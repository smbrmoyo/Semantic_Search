{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic_search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1LUFtFTPdnnEakZaf3GGT8mBMBBAJVj-F",
      "authorship_tag": "ABX9TyM38xBF9NS2PzKTyp7r7C4W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smbrmoyo/Semantic_Search/blob/main/Semantic_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyzd-Yk2k1NC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivqo6QBkpc6p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d831998e-acde-4d9b-ed89-fa32b7a03fcf"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define the category map\n",
        "category_map = {'talk.politics.misc': 'Politics', 'rec.autos': 'Autos', \n",
        "        'rec.sport.hockey': 'Hockey', 'sci.electronics': 'Electronics', \n",
        "        'sci.med': 'Medicine'}\n",
        "\n",
        "# Get the training dataset\n",
        "training_data = fetch_20newsgroups(subset='train', \n",
        "        categories=category_map.keys(), shuffle=True, random_state=5)\n",
        "\n",
        "# Build a count vectorizer and extract term counts \n",
        "count_vectorizer = CountVectorizer()\n",
        "train_tc = count_vectorizer.fit_transform(training_data.data)\n",
        "print(\"\\nDimensions of training data:\", train_tc.shape)\n",
        "\n",
        "# Create the tf-idf transformer\n",
        "tfidf = TfidfTransformer()\n",
        "train_tfidf = tfidf.fit_transform(train_tc)\n",
        "\n",
        "# Define test data \n",
        "input_data = [\n",
        "    'You need to be careful with cars when you are driving on slippery roads', \n",
        "    'A lot of devices can be operated wirelessly',\n",
        "    'Players need to be careful when they are close to goal posts',\n",
        "    'Political debates help us understand the perspectives of both sides'\n",
        "]\n",
        "\n",
        "# Train a Multinomial Naive Bayes classifier\n",
        "classifier = MultinomialNB().fit(train_tfidf, training_data.target)\n",
        "\n",
        "# Transform input data using count vectorizer\n",
        "input_tc = count_vectorizer.transform(input_data)\n",
        "\n",
        "# Transform vectorized data using tfidf transformer\n",
        "input_tfidf = tfidf.transform(input_tc)\n",
        "\n",
        "# Predict the output categories\n",
        "predictions = classifier.predict(input_tfidf)\n",
        "\n",
        "# Print the outputs\n",
        "for sent, category in zip(input_data, predictions):\n",
        "    print('\\nInput:', sent, '\\nPredicted category:', \\\n",
        "            category_map[training_data.target_names[category]])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dimensions of training data: (2844, 40321)\n",
            "\n",
            "Input: You need to be careful with cars when you are driving on slippery roads \n",
            "Predicted category: Autos\n",
            "\n",
            "Input: A lot of devices can be operated wirelessly \n",
            "Predicted category: Electronics\n",
            "\n",
            "Input: Players need to be careful when they are close to goal posts \n",
            "Predicted category: Hockey\n",
            "\n",
            "Input: Political debates help us understand the perspectives of both sides \n",
            "Predicted category: Politics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8uu8tlRfeV_"
      },
      "source": [
        "# Optimized **Chatbot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-87VGEj-kpU"
      },
      "source": [
        "!pip install tflearn\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TVesUkwLqJ-",
        "outputId": "0b863f15-dcb1-41b5-d343-24eb5ec14f62"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "import numpy\n",
        "import tflearn\n",
        "import tensorflow\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode\n",
        "\n",
        "\n",
        "\n",
        "class VoteClassifier(ClassifierI):\n",
        "    def __init__(self, *classifiers):\n",
        "        self._classifiers = classifiers\n",
        "\n",
        "    def classify(self, features):\n",
        "        votes = []\n",
        "        for c in self._classifiers:\n",
        "            v = c.classify(features)\n",
        "            votes.append(v)\n",
        "        return mode(votes)\n",
        "\n",
        "    def confidence(self, features):\n",
        "        votes = []\n",
        "        for c in self._classifiers:\n",
        "            v = c.classify(features)\n",
        "            votes.append(v)\n",
        "\n",
        "        choice_votes = votes.count(mode(votes))\n",
        "        conf = choice_votes / len(votes)\n",
        "        return conf\n",
        "\n",
        "doc_greeting = open(\"drive/MyDrive/Greeting.txt\",\"r\").read()\n",
        "doc_goodbye = open(\"drive/MyDrive/Goodbye.txt\",\"r\").read()\n",
        "doc_appointment = open(\"drive/MyDrive/Appointment.txt\",\"r\").read()\n",
        "doc_emergency = open(\"drive/MyDrive/Emergency.txt\",\"r\").read()\n",
        "doc_find = open(\"drive/MyDrive/Find_hospital.txt\",\"r\").read()\n",
        "\n",
        "documents = []\n",
        "\n",
        "for r in doc_greeting.split('\\n'):\n",
        "    documents.append( (r, \"Greeting\") )\n",
        "\n",
        "for r in doc_goodbye.split('\\n'):\n",
        "    documents.append( (r, \"Goodbye\") )\n",
        "\n",
        "for r in doc_appointment.split('\\n'):\n",
        "    documents.append( (r, \"Appointment\") )\n",
        "\n",
        "for r in doc_emergency.split('\\n'):\n",
        "    documents.append( (r, \"Emergency\") )\n",
        "\n",
        "for r in doc_find.split('\\n'):\n",
        "    documents.append( (r, \"Find\") )\n",
        "\n",
        "all_words = []\n",
        "\n",
        "doc_greeting_words = word_tokenize(doc_greeting)\n",
        "doc_goodbye_words = word_tokenize(doc_goodbye)\n",
        "doc_appointment_words = word_tokenize(doc_appointment)\n",
        "doc_emergency_words = word_tokenize(doc_emergency)\n",
        "doc_find_words = word_tokenize(doc_find)\n",
        "\n",
        "for w in doc_greeting_words:\n",
        "    all_words.append(w.lower())\n",
        "for w in doc_goodbye_words:\n",
        "    all_words.append(w.lower())\n",
        "for w in doc_appointment_words:\n",
        "    all_words.append(w.lower())\n",
        "for w in doc_emergency_words:\n",
        "    all_words.append(w.lower())\n",
        "for w in doc_find_words:\n",
        "    all_words.append(w.lower())\n",
        "\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "\n",
        "#Next, we also need to adjust our feature finding function,\n",
        "# mainly tokenizing by word in the document, since we didn't have a nifty .words() feature for our new sample.\n",
        "# I also went ahead and increased the most common words:\n",
        "\n",
        "word_features = list(all_words.keys())\n",
        "\n",
        "def find_features(document):\n",
        "    words = word_tokenize(document)\n",
        "    features = {}\n",
        "\n",
        "    for w in word_features:\n",
        "        features[w] = (w in words)\n",
        "\n",
        "    return features\n",
        "\n",
        "#ORIGINAL LINE\t\n",
        "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
        "random.shuffle(featuresets)\n",
        "\n",
        "training_set = featuresets\n",
        "testing_set =  featuresets[-5:]\n",
        "\n",
        "#print(documents[0])\n",
        "#print(testing_set[-1])\n",
        "print(\"length training set\" , len(training_set))\n",
        "print(\"length testing set\" , len(testing_set))\n",
        "print(\"length documents\" , len(documents))\n",
        "print(\"length word_features\" , len(word_features))\n",
        "print(\"length all_words\" , len(all_words))\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
        "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
        "#classifier.show_most_informative_features(15)\n",
        "\n",
        "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
        "MNB_classifier.train(training_set)\n",
        "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
        "\n",
        "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
        "LogisticRegression_classifier.train(training_set)\n",
        "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
        "\n",
        "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
        "SGDClassifier_classifier.train(training_set)\n",
        "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
        "\n",
        "voted_classifier = VoteClassifier(\n",
        "                                  MNB_classifier,\n",
        "                                  LogisticRegression_classifier,\n",
        "                                  LogisticRegression_classifier)\n",
        "\n",
        "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length training set 55\n",
            "length testing set 5\n",
            "length documents 55\n",
            "length word_features 80\n",
            "length all_words 80\n",
            "Original Naive Bayes Algo accuracy percent: 100.0\n",
            "MNB_classifier accuracy percent: 100.0\n",
            "LogisticRegression_classifier accuracy percent: 100.0\n",
            "SGDClassifier_classifier accuracy percent: 100.0\n",
            "voted_classifier accuracy percent: 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvR3-DZpcJW2"
      },
      "source": [
        "**FUNCTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txhJvV50Syz3"
      },
      "source": [
        "#FUNCTIONS\n",
        "\n",
        "def func_emergency():\n",
        "  print(\"We are sending an ambulance. Could you give us your location?\")\n",
        "\n",
        "def func_appointment():\n",
        "  print(\"What date best suits you?\")\n",
        "\n",
        "def func_find():\n",
        "  print(\"Looking for the closest hospital \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Ul5my8cHIX"
      },
      "source": [
        "**TAGGING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLnz5LGBL01f"
      },
      "source": [
        "def tagging(text):\n",
        "    feats = find_features(text)\n",
        "    return voted_classifier.classify(feats),voted_classifier.confidence(feats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuEM3-2UcCNC"
      },
      "source": [
        "**MAIN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwTnw4YDENHK",
        "outputId": "0efa2c24-902a-4fc0-ff9c-3f1f8c0b10f3"
      },
      "source": [
        "answer, confiance = tagging(\"I am hurt. I need help\")\n",
        "#print(\"Tag:\",answer,\"--- Confidence:\", confiance)\n",
        "if confiance >= 0.8:\n",
        "  if answer == 'Emergency':\n",
        "    func_emergency()\n",
        "  elif answer == 'Appointment':\n",
        "    func_appointment()\n",
        "  elif answer == 'Find':\n",
        "    func_find()\n",
        "  else: print(\"che ne compwen pa\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are sending an ambulance. Could you give us your location?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK4_LhlYjeCh"
      },
      "source": [
        "ADDING **VOCABULARY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RdKzhjMMN7y",
        "outputId": "6f6cecb2-f1a2-4012-c61e-a61e5385b99d"
      },
      "source": [
        "#textwords = [w.lower() for w in word_tokenize('Emergency.json')]\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "ignored_words = set(stopwords.words('english'))\n",
        "print(ignored_words)\n",
        "\n",
        "filterstops = lambda i: len(i) < 3 or i in ignored_words\n",
        "print(filterstops)\n",
        "\n",
        "finder = BigramCollocationFinder.from_words(doc_emergency_words)\n",
        "\n",
        "finder.apply_word_filter(filterstops)\n",
        "\n",
        "print(finder.nbest(BigramAssocMeasures.likelihood_ratio, 10))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'by', 'mightn', 'until', 'aren', 'isn', 'so', 'his', 'theirs', 'weren', 'we', 'to', 'of', 'having', 'against', 'why', 'both', 'did', \"hasn't\", 'before', 'what', 'does', 'who', 'yourself', 'shan', 'each', 'and', 'wasn', 'doesn', 'further', 'its', 'because', 'up', 'these', \"hadn't\", 'had', 'in', 'any', 'over', 'or', 'if', 'll', 'on', 'him', 'hasn', 'above', 'her', 'being', 'won', 'as', \"mustn't\", \"should've\", 'but', \"it's\", 'she', \"you're\", 'a', \"couldn't\", 'be', 'ourselves', 'are', \"don't\", 'm', 'through', 't', 'only', 'very', 'down', 'out', 'after', 'which', 'our', 'whom', \"won't\", 'have', 'were', \"haven't\", 'from', 'yours', 'just', 'no', 'haven', 'o', 'it', 'the', 'again', \"weren't\", \"shouldn't\", 'them', 'needn', 're', 'this', \"wouldn't\", 'my', 'themselves', 'same', 'you', 'than', 'for', \"she's\", 'can', 'below', 'those', 'with', 'nor', 'few', 'been', 'when', 'they', 'myself', 'didn', 'shouldn', 'more', 'itself', 'hadn', 'was', 'too', \"aren't\", 'ain', 'has', 'herself', \"didn't\", 'do', 'that', \"that'll\", 'between', 'at', 'yourselves', 'once', 'should', 'is', 'i', \"you'd\", 'then', 'there', 'ours', 've', 'mustn', 'd', 'wouldn', 'most', 'such', 'am', \"wasn't\", 'all', 'here', 'y', 'how', 'don', 'an', 'their', 'into', 'own', 'where', 'not', 'under', 'during', \"shan't\", 'your', 'will', 'while', 'doing', \"isn't\", 'himself', \"mightn't\", 'other', 'hers', 'he', 'me', 'couldn', 'about', \"you'll\", 's', 'off', 'now', \"doesn't\", 'ma', \"you've\", 'some', \"needn't\"}\n",
            "<function <lambda> at 0x7fb6c6cd2598>\n",
            "[('blood', 'everywhere'), ('medic', 'come'), ('pressing', 'please'), ('would', 'like'), ('ambulance', 'Someone'), ('come', 'help'), ('please', 'help'), ('Someone', 'call'), ('assistance', '911'), ('bleeding', 'someone')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgXyVXiiN6Y2"
      },
      "source": [
        "\n",
        "'''word_1 = \"blood\"\n",
        "targets = wordnet.synsets(word_1)\n",
        "print(targets)\n",
        "\n",
        "syn_1 = targets[0]\n",
        "print(syn_1.definition())\n",
        "print(syn_1.hypernym_paths())\n",
        "print(syn_1.hyponyms())\n",
        "\n",
        "syn_1.lemmas()[0]\n",
        "print(\"Lemmas: \" , syn_1)\n",
        "\n",
        "synArr = []\n",
        "antArr = []\n",
        "\n",
        "for lem in syn_1.lemmas():\n",
        "  synArr.append(lem.name())\n",
        "\n",
        "print(synArr)\n",
        "\n",
        "synArr = set(synArr)\n",
        "\n",
        "print(synArr)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl3Zte8eTPnN"
      },
      "source": [
        "**VOCABULARY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5q3BDWE6ZEE"
      },
      "source": [
        "Appointment\n",
        "I would like to take an appointment\n",
        "I have an appointment\n",
        "when can I take an appointment\n",
        "I am an apointee\n",
        "when is the doctor available\n",
        "Do I have an appointment\n",
        "when can I get an appointment\n",
        "I need a session\n",
        "I want a session\n",
        "\n",
        "I would like to call 911\n",
        "I injured myself\n",
        "I harmed myself\n",
        "I was harmed\n",
        "I need some assistance\n",
        "911\n",
        "I cut myself\n",
        "Is it 911\n",
        "It is an emergency\n",
        "it is urgent\n",
        "it is pressing\n",
        "please help\n",
        "I need aid\n",
        "aid someone\n",
        "I need a doctor\n",
        "I need a medic\n",
        "come help me\n",
        "somebody is bleeding\n",
        "someone is injured\n",
        "someone is hemorrhaging\n",
        "call an ambulance\n",
        "Someone call an ambulance\n",
        "there is blood everywhere\n",
        "\n",
        "Find nearest hospital\n",
        "Find the emergencies\n",
        "Where is the hospital\n",
        "The closest hospital\n",
        "detect where the closest hospital is\n",
        "find the way to the closest hospital\n",
        "find the next hospital\n",
        "notice where the closest hospital is\n",
        "my wife is giving birth"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}